{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os, json\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.tree import export_graphviz \n",
    "import graphviz\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func that reads the ''category'' column (from JSON format)\n",
    "def CustomParser1(df):\n",
    "    j1 = json.loads(df)\n",
    "    return j1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part loads as many files as instructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files (n):\n",
    "    df=pd.DataFrame()\n",
    "    read_df=pd.DataFrame()\n",
    "    a=[\"%03d\" % x for x in range(n)]\n",
    "    for filenum in a:\n",
    "        filename='Data/Kickstarter'+filenum+'.csv'\n",
    "        read_df=pd.read_csv(filename,converters={'category':CustomParser1},header=0)\n",
    "        df=pd.concat([df,read_df],ignore_index=True)\n",
    "        \n",
    "    return df\n",
    "Filesnum=input('How many file should I load? 1..54  ')\n",
    "df=load_files (int(Filesnum))\n",
    "df['campaign_name']=df['name']\n",
    "df.to_csv('data/data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv('Data/Kickstarter.csv',converters={'category':CustomParser1},header=0)\n",
    "#make differnt columns out of ''category'' format\n",
    "df[sorted(df['category'][0].keys(),reverse=False)] = df['category'].apply(pd.Series) \n",
    "df1=df[['category','color','parent_id','urls','id','name','position']]\n",
    "df['category.parent_id']=df1['position']\n",
    "df['category.id']=df1['color']\n",
    "df['category.position']=df1['parent_id']\n",
    "df['category.name']=df1['id']\n",
    "df['category.slug']=df1['name']\n",
    "#split ''slug'' and leaves just the main category name\n",
    "df['category.slug']=df['category.slug'].apply(lambda x: x.split('/'))\n",
    "df['category.slug']=df['category.slug'].apply(lambda x: x.pop(0))\n",
    "df[['category','category.parent_id','category.id','category.name','category.position','category.slug']][:1]\n",
    "df['name']=df['campaign_name']\n",
    "df=df.drop(['campaign_name'],axis=1)\n",
    "df.to_csv('data/data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func that reads the ''creator'' column (from JSON format).\n",
    "#some of the cells cause problems \n",
    "    #for exemple- the cell JSON format include double apostrophes in nicknames like \"Elad \"Superman\" Toister\" confused it.\n",
    "\n",
    "def CustomParser2(df2):\n",
    "    try:\n",
    "        j2 = json.loads(df2)\n",
    "        return j2\n",
    "    except: #the func pass all the errored rows and return 0 to the \"creator\" columnn. \n",
    "        return 0\n",
    "    pass\n",
    "            \n",
    "df2=pd.read_csv('data/data.csv',converters={'creator':CustomParser2},header=0)\n",
    "#count and collect all the droped rows- so we can know the \"cost\" of te dropping (and maybe i will succed to solve it in the future)\n",
    "droped=df2.loc[df2['creator']==0,['creator']]\n",
    "df2=df2.loc[df2['creator']!=0]\n",
    "drop_list=list(droped.index)\n",
    "#df['creator'].iloc[drop_list]=df['creator'].iloc[drop_list].apply(lambda x: x.replace(' ',',')) is a start of a solution\n",
    "df=df.drop(index=drop_list)\n",
    "print('droped rows:',len(drop_list))\n",
    "print (len(df))\n",
    "print (len(df2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 func that make diffent columns out of \"creator\" column (the auto func i used before don't works here. i did it manually)\n",
    "df2['creator_name']=df2['creator'].apply(lambda x: x['name'])\n",
    "df2['creator_id']=df2['creator'].apply(lambda x: x['id'])\n",
    "#\"inject\" it back to the original df\n",
    "df['creator_name']=df2['creator_name']\n",
    "df['creator_id']=df2['creator_id']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning func\n",
    "def clean(df):\n",
    "    data = df.copy()\n",
    "    #this is important beacuse this is the part we decide  which columns entered the data set.\n",
    "    #the main structure is like in the exemple but i manipulate and add some additional columns i think we need to include(*marked) . \n",
    "    selected_cols = ['creator_name',\n",
    "                     'name',\n",
    "                     'creator_id', #*\n",
    "                     'backers_count',\n",
    "                     'blurb',\n",
    "                     'is_starred', #*\n",
    "                     'category.id', #*\n",
    "                     'category.name',\n",
    "                     'category.parent_id',\n",
    "                     'category.slug',\n",
    "                     'country',\n",
    "                     'created_at',\n",
    "                     'currency',  \n",
    "                     'deadline',\n",
    "                     'goal',\n",
    "                     'launched_at',\n",
    "                     'staff_pick',\n",
    "                     'state',\n",
    "                     'usd_pledged',\n",
    "                     'usd_type']\n",
    "    data = data[selected_cols]\n",
    "    data['is_starred']=data['is_starred'].replace({1: True , None: False})\n",
    "    data = data.dropna()\n",
    "    successful = data['state'] == \"successful\"\n",
    "    failed = data['state'] == \"failed\"\n",
    "    cancelled = data['state'] == \"cancelled\"\n",
    "    suspended = data['state'] == \"suspended\"\n",
    "    data = data.loc[failed | successful | cancelled | suspended]\n",
    "    num_cols = ['usd_pledged',\n",
    "                'deadline',\n",
    "                'created_at',\n",
    "                'launched_at']\n",
    "    data[num_cols] = data[num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    data['created_at'] = pd.to_datetime(data['created_at'],unit='s')\n",
    "    data['launched_at'] = pd.to_datetime(data['launched_at'],unit='s')\n",
    "    data['deadline'] = pd.to_datetime(data['deadline'],unit='s')\n",
    "    return data\n",
    "\n",
    "data = clean(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create data frames containing only single main category\n",
    "\n",
    "categories=data['category.slug'].unique()\n",
    "frames = {}\n",
    "for ct in categories:\n",
    "    frames[ct] = data[data['category.slug'] == ct]\n",
    "#We will use Progressbar to track progress as it istime consuming operation\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(331675)\n",
    "\n",
    "\n",
    "def getElementsInRange(cat,end,week):\n",
    "    '''Get number of launched projects in given range from (end - week) to end'''\n",
    "    global pbar\n",
    "    pob = frames[cat]\n",
    "    start = end - pd.DateOffset(weeks = week)\n",
    "    # as we sorted our projects by launch date earlier geting number of projects in given date range is easy\n",
    "    value = pob['launched_at'].searchsorted(end)[0] - pob['launched_at'].searchsorted(start)[0]\n",
    "    pbar.update()\n",
    "    return value\n",
    "# Number of projects in same category for last week    \n",
    "data['Last_Week'] = data.apply(lambda x: getElementsInRange(x['category.slug'],x['launched_at'],1),axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = pyprind.ProgBar(331675)\n",
    "# Number of projects in same category for last month    \n",
    "data['Last_Month'] = data.apply(lambda x: getElementsInRange(x['category.slug'],x['launched_at'],4),axis = 1) \n",
    "\n",
    "pbar = pyprind.ProgBar(331675)\n",
    "# Number of projects in same category for last year    \n",
    "data['Last_Year'] =data.apply(lambda x: getElementsInRange(x['category.slug'],x['launched_at'],52),axis = 1) \n",
    "\n",
    "pbar = pyprind.ProgBar(331675)\n",
    "\n",
    "data['Last_3_Month'] = data.apply(lambda x: getElementsInRange(x['category.slug'],x['launched_at'],13),axis = 1)\n",
    "\n",
    "pbar = pyprind.ProgBar(331675)\n",
    "  \n",
    "data['Last_6_Month'] = data.apply(lambda x: getElementsInRange(x['category.slug'],x['launched_at'],26),axis = 1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def engineer_features(data):\n",
    "    #make state 1 or 0\n",
    "    data['state'].replace('suspended','failed',inplace=True)   \n",
    "    data['state_num'] = data['state'].apply(lambda x: 1 if x=='successful' else 0)\n",
    "    #time to reletive time\n",
    "    data['launched_at_hr'] = data['launched_at'].apply(lambda x: x.hour) + 1\n",
    "    data['launched_at_day_in_week'] = data['launched_at'].apply(lambda x: x.dayofweek + 1)\n",
    "    data['launched_at_day_in_month'] = data['launched_at'].apply(lambda x: x.day ) #Elad's comment\n",
    "    data['launched_at_mo'] = data['launched_at'].apply(lambda x: x.month)\n",
    "    data['launched_at_yr'] = data['launched_at'].apply(lambda x: x.year)\n",
    "    data['deadline_hr'] = data['deadline'].apply(lambda x: x.hour) + 1\n",
    "    data['deadline_day_in_week'] = data['deadline'].apply(lambda x: x.dayofweek + 1) \n",
    "    data['deadline_day_in_month'] = data['deadline'].apply(lambda x: x.day ) #Elad's comment\n",
    "    data['deadline_mo'] = data['deadline'].apply(lambda x: x.month)\n",
    "    data['deadline_yr'] = data['deadline'].apply(lambda x: x.year)\n",
    "    data['created_at_hr'] = data['created_at'].apply(lambda x: x.hour) + 1\n",
    "    data['created_at_day_in_week'] = data['created_at'].apply(lambda x: x.dayofweek + 1) \n",
    "    data['created_at_day_in_month'] = data['created_at'].apply(lambda x: x.day )  #Elad's comment\n",
    "    data['created_at_mo'] = data['created_at'].apply(lambda x: x.month)\n",
    "    data['created_at_yr'] = data['created_at'].apply(lambda x: x.year)\n",
    "    data['count'] = 1\n",
    "    #data['success'] = (data['state'] == 'successful')\n",
    "    data['launched-created'] = (data.launched_at - data.created_at).dt.components.days\n",
    "    data['deadline-launched'] = (data.deadline - data.launched_at).dt.components.days\n",
    "    data=data.drop(['launched_at','created_at','deadline','state'],axis=1) #drop original time col\n",
    "    data['pledge_perc']=data['usd_pledged']/data['goal']*100\n",
    "   \n",
    "    return data\n",
    "data = engineer_features(data)\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = pd.DataFrame()\n",
    "# length of the name\n",
    "data['name_len'] = data.name.str.len()\n",
    "# if name contains a question mark\n",
    "data['name_is_question'] = (data.name.str[-1] == '?').astype(int)\n",
    "# if name contains an exclamation mark\n",
    "data['name_is_exclamation'] = (data.name.str[-1] == '!').astype(int)\n",
    "# if name is uppercase\n",
    "data['name_is_upper'] = data.name.str.isupper().astype(float)\n",
    "def count_non_character(row):\n",
    "    '''Number of non character in the sentence'''\n",
    "    return sum((0 if c.isalpha() else 1 for c in str(row)))\n",
    "# number of non character in the name\n",
    "data['name_non_character'] = data.name.apply(count_non_character)\n",
    "# number of words in the name\n",
    "data['name_number_of_word'] = data.name.apply(lambda x: len(str(x).split(' ')))\n",
    "# We generate new feature based on ratio between vowels and other alpha characters\n",
    "def countVowelstoLettersRatio(s):\n",
    "    '''Count ratio between vowels and letters'''\n",
    "    s = str(s)\n",
    "    count = 1  \n",
    "    vowels = 0\n",
    "    for i in s:\n",
    "        if i.isalpha():\n",
    "            count = count + 1\n",
    "            if i in 'aeiou':\n",
    "                vowels = vowels + 1\n",
    "    return ((vowels * 1.0) / count)\n",
    "\n",
    "# for each name calculate vowels ratio\n",
    "data['name_vowel_ratio'] = data.name.apply(countVowelstoLettersRatio)\n",
    "\n",
    "#blurb\n",
    "data['blurb_number_of_word'] = data.blurb.apply(lambda x: len(str(x).split(' ')))\n",
    "data['blurb_vowel_ratio'] = data.blurb.apply(lambda x: len(str(x).split(' ')))\n",
    "data['blurb_non_character'] = data.blurb.apply(count_non_character)\n",
    "\n",
    "#goal split in 1000\\500\\10?\n",
    "data['goal_1000'] = data.goal.apply(lambda x: x // 1000)\n",
    "data['goal_500'] = data.goal.apply(lambda x: x // 500)\n",
    "data['goal_10'] = data.goal.apply(lambda x: x // 10)\n",
    "data.to_csv('Data/data.csv')\n",
    "\n",
    "data.info(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for creating a csv for the visualization:\n",
    "### sdata.csv=small dataset\n",
    "### sdata.csv=big dataset\n",
    "\n",
    "### run twice and then save the dataset for further handelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Data/data.csv')\n",
    "kk = pd.DataFrame()\n",
    "# length of the name\n",
    "data['name_len'] = data.name.str.len()\n",
    "# if name contains a question mark\n",
    "data['name_is_question'] = (data.name.str[-1] == '?').astype(int)\n",
    "# if name contains an exclamation mark\n",
    "data['name_is_exclamation'] = (data.name.str[-1] == '!').astype(int)\n",
    "# if name is uppercase\n",
    "data['name_is_upper'] = data.name.str.isupper().astype(float)\n",
    "def count_non_character(row):\n",
    "    '''Number of non character in the sentence'''\n",
    "    return sum((0 if c.isalpha() else 1 for c in str(row)))\n",
    "# number of non character in the name\n",
    "data['name_non_character'] = data.name.apply(count_non_character)\n",
    "# number of words in the name\n",
    "data['name_number_of_word'] = data.name.apply(lambda x: len(str(x).split(' ')))\n",
    "# We generate new feature based on ratio between vowels and other alpha characters\n",
    "def countVowelstoLettersRatio(s):\n",
    "    '''Count ratio between vowels and letters'''\n",
    "    s = str(s)\n",
    "    count = 1  \n",
    "    vowels = 0\n",
    "    for i in s:\n",
    "        if i.isalpha():\n",
    "            count = count + 1\n",
    "            if i in 'aeiou':\n",
    "                vowels = vowels + 1\n",
    "    return ((vowels * 1.0) / count)\n",
    "\n",
    "# for each name calculate vowels ratio\n",
    "data['name_vowel_ratio'] = data.name.apply(countVowelstoLettersRatio)\n",
    "\n",
    "#blurb\n",
    "data['blurb_number_of_word'] = data.blurb.apply(lambda x: len(str(x).split(' ')))\n",
    "data['blurb_vowel_ratio'] = data.blurb.apply(lambda x: len(str(x).split(' ')))\n",
    "data['blurb_non_character'] = data.blurb.apply(count_non_character)\n",
    "\n",
    "#goal split in 1000\\500\\10?\n",
    "data['goal_1000'] = data.goal.apply(lambda x: x // 1000)\n",
    "data['goal_500'] = data.goal.apply(lambda x: x // 500)\n",
    "data['goal_10'] = data.goal.apply(lambda x: x // 10)\n",
    "data=data.drop(['Unnamed: 0'],axis=1)\n",
    "#change the name for differnt sizes\n",
    "data.to_csv('Data/bdata.csv')\n",
    "data.info(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for creating a ML csv:\n",
    "### Sdata_for_ML.csv=small dataset\n",
    "### Bdata_for_ML.csv=big dataset\n",
    "\n",
    "### run twice and then save the dataset for further handelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies=['category.name', 'category.slug','country', 'currency','launched_at_day_in_week','deadline_day_in_week']\n",
    "mod1_columns=[ 'is_starred', 'category.name', 'category.slug',\n",
    "       'country', 'currency', 'goal','usd_pledged','pledge_perc', 'staff_pick', 'launched_at_hr',\n",
    "       'launched_at_day_in_week', 'launched_at_day_in_month', 'launched_at_mo',\n",
    "       'launched_at_yr', 'deadline_hr', 'deadline_day_in_week',\n",
    "       'deadline_day_in_month', 'deadline_mo', 'deadline_yr', 'created_at_hr',\n",
    "       'created_at_day_in_week', 'created_at_day_in_month', 'created_at_mo',\n",
    "       'created_at_yr', 'launched-created','goal_1000','goal_500','goal_10','Last_Week','Last_Month',\n",
    "       'Last_Year','Last_3_Month','Last_6_Month',\n",
    "       'deadline-launched','state_num','name_len','name_is_question','name_is_exclamation','name_is_upper','name_non_character','name_number_of_word','name_vowel_ratio','blurb_number_of_word','blurb_vowel_ratio','blurb_non_character']\n",
    "data=data[mod1_columns]\n",
    "data = pd.get_dummies(data, columns=dummies)\n",
    "data[\"deadline-launched\"]=np.log(data[\"deadline-launched\"])\n",
    "data['goal']=np.log(data['goal'])\n",
    "#change the name for differnt sizes\n",
    "data.to_csv('data/Bdata_for_ML.csv')  \n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# click ''Run all above\" <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in categories:\n",
    "    y=data.loc[data['category.slug']==x]\n",
    "    print(x,y[['Last_Week','Last_Month','Last_Year']].mean())\n",
    "\n",
    "def getRangeMean(cat,end,week):\n",
    "    global pbar\n",
    "    pob = frames[cat]\n",
    "    start = end - pd.DateOffset(weeks = week)\n",
    "    value = pob.iloc[pob['launched_at'].searchsorted(start)[0]:pob['launched_at'].searchsorted(end)[0]]['goal'].mean()\n",
    "    pbar.update()\n",
    "    return value\n",
    "pbar = pyprind.ProgBar(331675)\n",
    "# Mean goal for category last month\n",
    "data['mean_goal_in_category_last_month'] = data.apply(lambda x: getRangeMean(x['category.slug'],x['launched_at'],4),axis = 1) \n",
    "\n",
    "def getRangeMedian(cat,end,week):\n",
    "    global pbar\n",
    "    pob = frames[cat]\n",
    "    start = end - pd.DateOffset(weeks = week)\n",
    "    value = pob.iloc[pob['launched_at'].searchsorted(start)[0]:pob['launched_at'].searchsorted(end)[0]]['goal'].median()\n",
    "    pbar.update()\n",
    "    return value\n",
    "\n",
    "pbar = pyprind.ProgBar(331675)\n",
    "# Median goal for category last month\n",
    "data['median_goal_in_category_last_month'] = data.apply(lambda x: getRangeMedian(x['category.slug'],x['launched_at'],4),axis = 1)\n",
    "\n",
    "pbar = pyprind.ProgBar(331675)\n",
    "# Mean goal for category last month\n",
    "data['mean_goal_in_category_last_year'] = data.apply(lambda x: getRangeMean(x['category.slug'],x['launched_at'],52),axis = 1) \n",
    "\n",
    "\n",
    "pbar = pyprind.ProgBar(331675)\n",
    "# Median goal in category last month\n",
    "data['median_goal_in_category_last_year'] = data.apply(lambda x: getRangeMedian(x['category.slug'],x['launched_at'],52),axis = 1) \n",
    "\n",
    "data['median_goal_Last_6_Month'] = data.apply(lambda x: getRangeMedian(x['category.slug'],x['launched_at'],26),axis = 1)\n",
    "\n",
    "data['mean_goal_Last_6_Month'] = data.apply(lambda x: getRangeMean(x['category.slug'],x['launched_at'],26),axis = 1)\n",
    "data['mean_goal_Last_Week'] = data.apply(lambda x: getRangeMean(x['category.slug'],x['launched_at'],1),axis = 1)\n",
    "data['median_goal_Last_Week'] = data.apply(lambda x: getRangeMedian(x['category.slug'],x['launched_at'],1),axis = 1)\n",
    "data.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
